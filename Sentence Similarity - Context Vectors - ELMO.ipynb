{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Similarity with Context Vectors - ELMO\n",
    "\n",
    "\n",
    "Deep Contextualized Word Representations (ELMo) have recently improved the state of the art in word embeddings by a noticeable amount. They were developed by the Allen institute for AI and will be presented at NAACL 2018 in early June. \n",
    "\n",
    "This notebook explores how the ELMo vectors can be used to calculate the sentence similarity. It will be evaluated here using SICK and STS data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "#### STS Benchmark\n",
    "The STS Benchmark brings together the English data from the SemEval sentence similarity tasks between 2012 and 2017. The data is split in training, development and test data: http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_1</th>\n",
       "      <th>sent_2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A girl is styling her hair.</td>\n",
       "      <td>A girl is brushing her hair.</td>\n",
       "      <td>2.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A group of men play soccer on the beach.</td>\n",
       "      <td>A group of boys are playing soccer on the beach.</td>\n",
       "      <td>3.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One woman is measuring another woman's ankle.</td>\n",
       "      <td>A woman measures another woman's ankle.</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A man is cutting up a cucumber.</td>\n",
       "      <td>A man is slicing a cucumber.</td>\n",
       "      <td>4.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A man is playing a harp.</td>\n",
       "      <td>A man is playing a keyboard.</td>\n",
       "      <td>1.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A woman is cutting onions.</td>\n",
       "      <td>A woman is cutting tofu.</td>\n",
       "      <td>1.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A man is riding an electric bicycle.</td>\n",
       "      <td>A man is riding a bicycle.</td>\n",
       "      <td>3.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A man is playing the drums.</td>\n",
       "      <td>A man is playing the guitar.</td>\n",
       "      <td>2.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A man is playing guitar.</td>\n",
       "      <td>A lady is playing the guitar.</td>\n",
       "      <td>2.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A man is playing a guitar.</td>\n",
       "      <td>A man is playing a trumpet.</td>\n",
       "      <td>1.714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          sent_1  \\\n",
       "0                    A girl is styling her hair.   \n",
       "1       A group of men play soccer on the beach.   \n",
       "2  One woman is measuring another woman's ankle.   \n",
       "3                A man is cutting up a cucumber.   \n",
       "4                       A man is playing a harp.   \n",
       "5                     A woman is cutting onions.   \n",
       "6           A man is riding an electric bicycle.   \n",
       "7                    A man is playing the drums.   \n",
       "8                       A man is playing guitar.   \n",
       "9                     A man is playing a guitar.   \n",
       "\n",
       "                                             sent_2    sim  \n",
       "0                      A girl is brushing her hair.  2.500  \n",
       "1  A group of boys are playing soccer on the beach.  3.600  \n",
       "2           A woman measures another woman's ankle.  5.000  \n",
       "3                      A man is slicing a cucumber.  4.200  \n",
       "4                      A man is playing a keyboard.  1.500  \n",
       "5                          A woman is cutting tofu.  1.800  \n",
       "6                        A man is riding a bicycle.  3.500  \n",
       "7                      A man is playing the guitar.  2.200  \n",
       "8                     A lady is playing the guitar.  2.200  \n",
       "9                       A man is playing a trumpet.  1.714  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from preprocessing.load_data import download_and_load_sts_data, download_and_load_sick_dataset\n",
    "\n",
    "sts_dev, sts_test = download_and_load_sts_data()\n",
    "sts_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SICK data\n",
    "The SICK dataset contains 10,000 English sentence pairs labelled with their semantic relatedness and entailment relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>sent_1</th>\n",
       "      <th>sent_2</th>\n",
       "      <th>sim</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>A group of boys in a yard is playing and a man...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A group of children is playing in the house an...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>3.2</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>The kids are playing outdoors near a man with ...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>The kids are playing outdoors near a man with ...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>3.4</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>3.7</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>Two dogs are fighting</td>\n",
       "      <td>Two dogs are wrestling and hugging</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14</td>\n",
       "      <td>A brown dog is attacking another animal in fro...</td>\n",
       "      <td>Two dogs are fighting</td>\n",
       "      <td>3.5</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>18</td>\n",
       "      <td>A brown dog is attacking another animal in fro...</td>\n",
       "      <td>Two dogs are wrestling and hugging</td>\n",
       "      <td>3.2</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25</td>\n",
       "      <td>Nobody is riding the bicycle on one wheel</td>\n",
       "      <td>A person in a black jacket is doing tricks on ...</td>\n",
       "      <td>2.8</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>26</td>\n",
       "      <td>A person is riding the bicycle on one wheel</td>\n",
       "      <td>A man in a black jacket is doing tricks on a m...</td>\n",
       "      <td>3.7</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  idx                                             sent_1  \\\n",
       "0   1  A group of kids is playing in a yard and an ol...   \n",
       "1   2  A group of children is playing in the house an...   \n",
       "2   3  The young boys are playing outdoors and the ma...   \n",
       "3   5  The kids are playing outdoors near a man with ...   \n",
       "4   9  The young boys are playing outdoors and the ma...   \n",
       "5  12                              Two dogs are fighting   \n",
       "6  14  A brown dog is attacking another animal in fro...   \n",
       "7  18  A brown dog is attacking another animal in fro...   \n",
       "8  25          Nobody is riding the bicycle on one wheel   \n",
       "9  26        A person is riding the bicycle on one wheel   \n",
       "\n",
       "                                              sent_2  sim       label  \n",
       "0  A group of boys in a yard is playing and a man...  4.5     NEUTRAL  \n",
       "1  A group of kids is playing in a yard and an ol...  3.2     NEUTRAL  \n",
       "2  The kids are playing outdoors near a man with ...  4.7  ENTAILMENT  \n",
       "3  A group of kids is playing in a yard and an ol...  3.4     NEUTRAL  \n",
       "4  A group of kids is playing in a yard and an ol...  3.7     NEUTRAL  \n",
       "5                 Two dogs are wrestling and hugging  4.0     NEUTRAL  \n",
       "6                              Two dogs are fighting  3.5     NEUTRAL  \n",
       "7                 Two dogs are wrestling and hugging  3.2     NEUTRAL  \n",
       "8  A person in a black jacket is doing tricks on ...  2.8     NEUTRAL  \n",
       "9  A man in a black jacket is doing tricks on a m...  3.7     NEUTRAL  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sick_all, sick_train, sick_test, sick_dev = download_and_load_sick_dataset()\n",
    "sick_all[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Frequencies\n",
    "To weight the word vectors we need frequency stat. For that we used word frequencies that have been collected from Wikipedia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility.frequency_loader import load_frequencies, load_doc_frequencies\n",
    "\n",
    "frequency = load_frequencies(\"/data/frequencies/frequencies.tsv\")\n",
    "doc_frequency = load_doc_frequencies(\"/data/frequencies/doc_frequencies.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Embeddings\n",
    "word2vec, elmo, bert, flair and stacked embeddings were used for these experiments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings.load_embeddings import load_word2vec\n",
    "word2vec = load_word2vec(\"/data/word2vec/GoogleNews-vectors-negative300.bin.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Elmo vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import ELMoEmbeddings\n",
    "\n",
    "elmo = ELMoEmbeddings('original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Average\n",
    "As our first similarity measure we are going to use vector average. Then take the embeddings of the words and calcluate the average to get a vector for the sentence. There are few benchmarks we created.\n",
    "1. Calculating the average considering all the words. \n",
    "2. Calculating the average removing stop words.\n",
    "3. Calculating the average while weighting the words with inverse document frequency.\n",
    "4. Calculating the average while weighting the words with inverse document frequency and removing the stop words. \n",
    "\n",
    "These experiments were done using word2vec and ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools as ft\n",
    "from preprocessing.normalize import normalize\n",
    "from matrices.word_vectors.avg_embeddings import run_avg_benchmark\n",
    "from utility.run_experiment import run_experiment\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matrices.context_vectors.avg_embeddings import run_context_avg_benchmark\n",
    "\n",
    "benchmarks = [(\"AVG-W2V\", ft.partial(run_avg_benchmark, model=word2vec, use_stoplist=False)),\n",
    "              (\"AVG-ELMO\", ft.partial(run_context_avg_benchmark, model=elmo, use_stoplist=False)),\n",
    "              (\"AVG-W2V-STOP\", ft.partial(run_avg_benchmark, model=word2vec, use_stoplist=True)),\n",
    "              (\"AVG-ELMO-STOP\", ft.partial(run_context_avg_benchmark, model=elmo, use_stoplist=True)),\n",
    "              (\"AVG-W2V-TFIDF\", ft.partial(run_avg_benchmark, model=word2vec, use_stoplist=False, doc_freqs=doc_frequency)),\n",
    "              (\"AVG-ELMO-TFIDF\", ft.partial(run_context_avg_benchmark, model=elmo, use_stoplist=False, doc_freqs=doc_frequency)),\n",
    "              (\"AVG-W2V-TFIDF-STOP\",ft.partial(run_avg_benchmark, model=word2vec, use_stoplist=True, doc_freqs=doc_frequency)),\n",
    "              (\"AVG-ELMO-TFIDF-STOP\",ft.partial(run_context_avg_benchmark, model=elmo, use_stoplist=True, doc_freqs=doc_frequency))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG-W2V\n",
      "RMSE=0.258\n",
      "$Pearson Correlation=0.732$\n",
      "$Spearman Correlation=0.624$\n",
      "AVG-ELMO\n",
      "RMSE=0.273\n",
      "$Pearson Correlation=0.654$\n",
      "$Spearman Correlation=0.591$\n",
      "AVG-W2V-STOP\n",
      "RMSE=0.240\n",
      "$Pearson Correlation=0.720$\n",
      "$Spearman Correlation=0.585$\n",
      "AVG-ELMO-STOP\n",
      "RMSE=0.263\n",
      "$Pearson Correlation=0.676$\n",
      "$Spearman Correlation=0.596$\n",
      "AVG-W2V-TFIDF\n",
      "RMSE=0.229\n",
      "$Pearson Correlation=0.708$\n",
      "$Spearman Correlation=0.581$\n",
      "AVG-ELMO-TFIDF\n",
      "RMSE=0.253\n",
      "$Pearson Correlation=0.675$\n",
      "$Spearman Correlation=0.589$\n",
      "AVG-W2V-TFIDF-STOP\n",
      "RMSE=0.228\n",
      "$Pearson Correlation=0.705$\n",
      "$Spearman Correlation=0.565$\n",
      "AVG-ELMO-TFIDF-STOP\n",
      "RMSE=0.249\n",
      "$Pearson Correlation=0.669$\n",
      "$Spearman Correlation=0.582$\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(25,20))\n",
    "row = 0\n",
    "column = 0\n",
    "sick_train = normalize(sick_train, [\"sim\"])\n",
    "for i in range(0, 8):\n",
    "    sims, topic = run_experiment(sick_train, benchmarks[i])\n",
    "    pearson_correlation = scipy.stats.pearsonr(sims, sick_train['sim'])[0]\n",
    "    spearman_correlation = scipy.stats.spearmanr(sims, sick_train['sim'])[0]\n",
    "    rmse = sqrt(mean_squared_error(sims, sick_train['sim']))\n",
    "    textstr = 'RMSE=%.3f\\n$Pearson Correlation=%.3f$\\n$Spearman Correlation=%.3f$'%(rmse, pearson_correlation, spearman_correlation)\n",
    "    sick_train['predicted_sim'] = pd.Series(sims).values\n",
    "    sick_train = sick_train.sort_values('sim')\n",
    "    id = list(range(0, len(sick_train.index)))\n",
    "    sick_train['id'] = pd.Series(id).values\n",
    "    \n",
    "    if(i < 4):\n",
    "        row = 0\n",
    "        column = i\n",
    "    if(i >= 4 and i < 8):\n",
    "        row = 1\n",
    "        column = i-4\n",
    "\n",
    "        \n",
    "    sick_train.plot(kind='scatter', x='id', y='sim',color='DarkBlue', label='Similarity', title = topic, ax=axes[row, column]);\n",
    "    sick_train.plot(kind='scatter', x='id', y='predicted_sim', color='DarkGreen', label='Predicted Similarity', ax=axes[row, column]);\n",
    "    axes[row, column].text(1500, 0, textstr, fontsize=12)\n",
    "    print (topic)\n",
    "    print (textstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth Inverse Frequency\n",
    "Taking the average of the word embeddings in a sentence, like we did above, is a very crude method of computing sentence embeddings. Most importantly, this gives far too much weight to words that are quite irrelevant, semantically speaking. Smooth Inverse Frequency tries to solve this problem.\n",
    "\n",
    "To compute SIF sentence embeddings, we first compute a weighted average of the token embeddings in the sentence. This procedure is very similar to the weighted average we used above, with the single difference that the word embeddings are weighted by a/a+p(w), where a is a parameter that is set to 0.001 by default, and p(w) is the estimated relative frequency of a word in a reference corpus.\n",
    "\n",
    "Next, we need to perform common component removal: we compute the principal component of the sentence embeddings we obtained above and subtract from them their projections on this first principal component. This corrects for the influence of high-frequency words that mostly have a syntactic or discourse function, such as \"just\", \"there\", \"but\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matrices.word_vectors.smooth_inverse_frequency import run_sif_benchmark\n",
    "from matrices.context_vectors.smooth_inverse_frequency import run_context_sif_benchmark\n",
    "import functools as ft\n",
    "\n",
    "benchmarks = [(\"SIF-W2V\", ft.partial(run_sif_benchmark, freqs=frequency, model=word2vec, use_stoplist=False)),\n",
    "              (\"SIF-ELMO\", ft.partial(run_context_sif_benchmark, freqs=frequency, model=elmo, use_stoplist=False))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIF-W2V\n",
      "rmse=0.204\n",
      "$Pearson Correlation=0.734$\n",
      "$Spearman Correlation=0.612$\n",
      "SIF-ELMO\n",
      "rmse=0.193\n",
      "$Pearson Correlation=0.697$\n",
      "$Spearman Correlation=0.616$\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from preprocessing.normalize import normalize\n",
    "from utility.run_experiment import run_experiment\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,10))\n",
    "row = 0\n",
    "column = 0\n",
    "sick_train = normalize(sick_train, [\"sim\"])\n",
    "for i in range(0, 2):\n",
    "    sims, topic = run_experiment(sick_train, benchmarks[i])\n",
    "    sick_train['predicted_sim'] = pd.Series(sims).values\n",
    "    sick_train = normalize(sick_train, [\"predicted_sim\"])\n",
    "    pearson_correlation = scipy.stats.pearsonr(sims, sick_train['sim'])[0]\n",
    "    spearman_correlation = scipy.stats.spearmanr(sims, sick_train['sim'])[0]\n",
    "    rmse = sqrt(mean_squared_error(sick_train['predicted_sim'], sick_train['sim']))\n",
    "    textstr = 'rmse=%.3f\\n$Pearson Correlation=%.3f$\\n$Spearman Correlation=%.3f$'%(rmse, pearson_correlation, spearman_correlation)    \n",
    "    sick_train = sick_train.sort_values('sim')\n",
    "    id = list(range(0, len(sick_train.index)))\n",
    "    sick_train['id'] = pd.Series(id).values\n",
    "    \n",
    "    if(i < 2):\n",
    "        column = i\n",
    "        \n",
    "    sick_train.plot(kind='scatter', x='id', y='sim',color='DarkBlue', label='Similarity', title = topic, ax=axes[column]);\n",
    "    sick_train.plot(kind='scatter', x='id', y='predicted_sim', color='DarkGreen', label='Predicted Similarity', ax=axes[column]);\n",
    "    axes[column].text(1500, 0.05, textstr, fontsize=12)\n",
    "    print (topic)\n",
    "    print (textstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Mover's Distance\n",
    "Word mover's distance is a popular alternative to the simple average embedding similarity. The Word Mover's Distance uses the word embeddings of the words in two texts to measure the minimum amount that the words in one text need to \"travel\" in semantic space to reach the words of the other text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matrices.word_vectors.word_movers_distance import run_wmd_benchmark\n",
    "from matrices.context_vectors.word_movers_distance import run_context_wmd_benchmark\n",
    "import functools as ft\n",
    "\n",
    "benchmarks = [(\"WMD-W2V\", ft.partial(run_wmd_benchmark, model=word2vec, use_stoplist=False)), \n",
    "              (\"WMD-ELMo\", ft.partial(run_context_wmd_benchmark, model=elmo, use_stoplist=False)),\n",
    "              (\"WMD-W2V-STOP\", ft.partial(run_wmd_benchmark, model=word2vec, use_stoplist=True)),    \n",
    "              (\"WMD-ELMo-STOP\", ft.partial(run_context_wmd_benchmark, model=elmo, use_stoplist=True))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WMD-W2V\n",
      "rmse=0.205\n",
      "$Pearson Correlation=0.642$\n",
      "$Spearman Correlation=0.593$\n",
      "WMD-ELMo\n",
      "rmse=0.220\n",
      "$Pearson Correlation=0.584$\n",
      "$Spearman Correlation=0.559$\n",
      "WMD-W2V-STOP\n",
      "rmse=0.215\n",
      "$Pearson Correlation=0.636$\n",
      "$Spearman Correlation=0.573$\n",
      "WMD-ELMo-STOP\n",
      "rmse=0.238\n",
      "$Pearson Correlation=0.600$\n",
      "$Spearman Correlation=0.549$\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from preprocessing.normalize import normalize\n",
    "from utility.run_experiment import run_experiment\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20,20))\n",
    "row = 0\n",
    "column = 0\n",
    "sick_train = normalize(sick_train, [\"sim\"])\n",
    "for i in range(0, 4):\n",
    "    sims, topic = run_experiment(sick_train, benchmarks[i])\n",
    "    sick_train['predicted_sim'] = pd.Series(sims).values\n",
    "    sick_train = normalize(sick_train, [\"predicted_sim\"])\n",
    "    pearson_correlation = scipy.stats.pearsonr(sims, sick_train['sim'])[0]\n",
    "    spearman_correlation = scipy.stats.spearmanr(sims, sick_train['sim'])[0]\n",
    "    rmse = sqrt(mean_squared_error(sick_train['predicted_sim'], sick_train['sim']))\n",
    "    textstr = 'rmse=%.3f\\n$Pearson Correlation=%.3f$\\n$Spearman Correlation=%.3f$'%(rmse, pearson_correlation, spearman_correlation)    \n",
    "    sick_train = sick_train.sort_values('sim')\n",
    "    id = list(range(0, len(sick_train.index)))\n",
    "    sick_train['id'] = pd.Series(id).values\n",
    "    \n",
    "    if(i < 2):\n",
    "        row = 0\n",
    "        column = i\n",
    "    if(i >= 2 and i < 4):\n",
    "        row = 1\n",
    "        column = i-2\n",
    "        \n",
    "    sick_train.plot(kind='scatter', x='id', y='sim',color='DarkBlue', label='Similarity', title = topic, ax=axes[row, column]);\n",
    "    sick_train.plot(kind='scatter', x='id', y='predicted_sim', color='DarkGreen', label='Predicted Similarity', ax=axes[row, column]);\n",
    "    axes[row, column].text(1500, 0.05, textstr, fontsize=12)\n",
    "    print (topic)\n",
    "    print (textstr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "Even though ELMo embeddings sound promising when it comes to sentence similarity task it didn't out performed most of the word2vec benchmarks. One reason might be the high dimensions in the ELMo embeddings. \n",
    "\n",
    "How ever in Smooth Inverse Frequency mthod ELMo embeddings outperformed word2vec embeddings - which is actually the best method we doscovered for simple sentence similarity. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sentence_similarity_3.6]",
   "language": "python",
   "name": "conda-env-sentence_similarity_3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

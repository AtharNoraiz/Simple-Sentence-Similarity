{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Similarity\n",
    "\n",
    "Word embeddings have become widespread in Natural Language Processing. They allow us to easily compute the semantic similarity between two words, or to find the words most similar to a target word. However, in many applications we're more interested in the similarity between two sentences or short texts. There are many ways to compare similarities of 2 texts using traditional methods like edit distance as well as using deep learning neural networks. In this notebook, I compare some simple ways of computing sentence similarity using word vectors and investigate how they perform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "#### STS Benchmark\n",
    "The STS Benchmark brings together the English data from the SemEval sentence similarity tasks between 2012 and 2017. The data is split in training, development and test data: http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_1</th>\n",
       "      <th>sent_2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A girl is styling her hair.</td>\n",
       "      <td>A girl is brushing her hair.</td>\n",
       "      <td>2.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A group of men play soccer on the beach.</td>\n",
       "      <td>A group of boys are playing soccer on the beach.</td>\n",
       "      <td>3.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One woman is measuring another woman's ankle.</td>\n",
       "      <td>A woman measures another woman's ankle.</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A man is cutting up a cucumber.</td>\n",
       "      <td>A man is slicing a cucumber.</td>\n",
       "      <td>4.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A man is playing a harp.</td>\n",
       "      <td>A man is playing a keyboard.</td>\n",
       "      <td>1.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A woman is cutting onions.</td>\n",
       "      <td>A woman is cutting tofu.</td>\n",
       "      <td>1.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A man is riding an electric bicycle.</td>\n",
       "      <td>A man is riding a bicycle.</td>\n",
       "      <td>3.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A man is playing the drums.</td>\n",
       "      <td>A man is playing the guitar.</td>\n",
       "      <td>2.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A man is playing guitar.</td>\n",
       "      <td>A lady is playing the guitar.</td>\n",
       "      <td>2.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A man is playing a guitar.</td>\n",
       "      <td>A man is playing a trumpet.</td>\n",
       "      <td>1.714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          sent_1  \\\n",
       "0                    A girl is styling her hair.   \n",
       "1       A group of men play soccer on the beach.   \n",
       "2  One woman is measuring another woman's ankle.   \n",
       "3                A man is cutting up a cucumber.   \n",
       "4                       A man is playing a harp.   \n",
       "5                     A woman is cutting onions.   \n",
       "6           A man is riding an electric bicycle.   \n",
       "7                    A man is playing the drums.   \n",
       "8                       A man is playing guitar.   \n",
       "9                     A man is playing a guitar.   \n",
       "\n",
       "                                             sent_2    sim  \n",
       "0                      A girl is brushing her hair.  2.500  \n",
       "1  A group of boys are playing soccer on the beach.  3.600  \n",
       "2           A woman measures another woman's ankle.  5.000  \n",
       "3                      A man is slicing a cucumber.  4.200  \n",
       "4                      A man is playing a keyboard.  1.500  \n",
       "5                          A woman is cutting tofu.  1.800  \n",
       "6                        A man is riding a bicycle.  3.500  \n",
       "7                      A man is playing the guitar.  2.200  \n",
       "8                     A lady is playing the guitar.  2.200  \n",
       "9                       A man is playing a trumpet.  1.714  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from preprocessing.load_data import download_and_load_sts_data, download_and_load_sick_dataset\n",
    "\n",
    "sts_dev, sts_test = download_and_load_sts_data()\n",
    "sts_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SICK data\n",
    "The SICK dataset contains 10,000 English sentence pairs labelled with their semantic relatedness and entailment relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>sent_1</th>\n",
       "      <th>sent_2</th>\n",
       "      <th>sim</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>A group of boys in a yard is playing and a man...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A group of children is playing in the house an...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>3.2</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>The kids are playing outdoors near a man with ...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>The kids are playing outdoors near a man with ...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>3.4</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>3.7</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>Two dogs are fighting</td>\n",
       "      <td>Two dogs are wrestling and hugging</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14</td>\n",
       "      <td>A brown dog is attacking another animal in fro...</td>\n",
       "      <td>Two dogs are fighting</td>\n",
       "      <td>3.5</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>18</td>\n",
       "      <td>A brown dog is attacking another animal in fro...</td>\n",
       "      <td>Two dogs are wrestling and hugging</td>\n",
       "      <td>3.2</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25</td>\n",
       "      <td>Nobody is riding the bicycle on one wheel</td>\n",
       "      <td>A person in a black jacket is doing tricks on ...</td>\n",
       "      <td>2.8</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>26</td>\n",
       "      <td>A person is riding the bicycle on one wheel</td>\n",
       "      <td>A man in a black jacket is doing tricks on a m...</td>\n",
       "      <td>3.7</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  idx                                             sent_1  \\\n",
       "0   1  A group of kids is playing in a yard and an ol...   \n",
       "1   2  A group of children is playing in the house an...   \n",
       "2   3  The young boys are playing outdoors and the ma...   \n",
       "3   5  The kids are playing outdoors near a man with ...   \n",
       "4   9  The young boys are playing outdoors and the ma...   \n",
       "5  12                              Two dogs are fighting   \n",
       "6  14  A brown dog is attacking another animal in fro...   \n",
       "7  18  A brown dog is attacking another animal in fro...   \n",
       "8  25          Nobody is riding the bicycle on one wheel   \n",
       "9  26        A person is riding the bicycle on one wheel   \n",
       "\n",
       "                                              sent_2  sim       label  \n",
       "0  A group of boys in a yard is playing and a man...  4.5     NEUTRAL  \n",
       "1  A group of kids is playing in a yard and an ol...  3.2     NEUTRAL  \n",
       "2  The kids are playing outdoors near a man with ...  4.7  ENTAILMENT  \n",
       "3  A group of kids is playing in a yard and an ol...  3.4     NEUTRAL  \n",
       "4  A group of kids is playing in a yard and an ol...  3.7     NEUTRAL  \n",
       "5                 Two dogs are wrestling and hugging  4.0     NEUTRAL  \n",
       "6                              Two dogs are fighting  3.5     NEUTRAL  \n",
       "7                 Two dogs are wrestling and hugging  3.2     NEUTRAL  \n",
       "8  A person in a black jacket is doing tricks on ...  2.8     NEUTRAL  \n",
       "9  A man in a black jacket is doing tricks on a m...  3.7     NEUTRAL  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sick_all, sick_train, sick_test, sick_dev = download_and_load_sick_dataset()\n",
    "sick_all[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Embeddings\n",
    "2 word embeddings were used for this experiment. Word2vec and Glove. \n",
    "\n",
    "#### Loading word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings.load_embeddings import load_word2vec\n",
    "word2vec = load_word2vec(\"/data/word2vec/GoogleNews-vectors-negative300.bin.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Glove\n",
    "To load Glove, we have to convert the downloaded GloVe file to word2vec format and then load the embeddings into a Gensim model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings.load_embeddings import load_glove\n",
    "glove = load_glove(\"/data/glove/glove.840B.300d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Frequencies\n",
    "To weight the word vectors we need frequency stat. For that we used word frequencies that have been collected from Wikipedia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility.frequency_loader import load_frequencies, load_doc_frequencies\n",
    "\n",
    "frequency = load_frequencies(\"/data/frequencies/frequencies.tsv\")\n",
    "doc_frequency = load_doc_frequencies(\"/data/frequencies/doc_frequencies.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Average\n",
    "As our first similarity measure we are going to use vector average. Then take the embeddings of the words and calcluate the average to get a vector for the sentence. There are few benchmarks we created.\n",
    "1. Calculating the average considering all the words. \n",
    "2. Calculating the average removing stop words.\n",
    "3. Calculating the average while weighting the words with inverse document frequency.\n",
    "4. Calculating the average while weighting the words with inverse document frequency and removing the stop words. \n",
    "\n",
    "These experiments were done using both word2vec and Glove. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools as ft\n",
    "from preprocessing.normalize import normalize\n",
    "from matrices.avg_embeddings import run_avg_benchmark\n",
    "from utility.run_experiment import run_experiment\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "benchmarks = [(\"AVG-W2V\", ft.partial(run_avg_benchmark, model=word2vec, use_stoplist=False)),\n",
    "              (\"AVG-W2V-STOP\", ft.partial(run_avg_benchmark, model=word2vec, use_stoplist=True)),\n",
    "              (\"AVG-W2V-TFIDF\", ft.partial(run_avg_benchmark, model=word2vec, use_stoplist=False, doc_freqs=doc_frequency)),\n",
    "              (\"AVG-W2V-TFIDF-STOP\",ft.partial(run_avg_benchmark, model=word2vec, use_stoplist=True, doc_freqs=doc_frequency)),\n",
    "              (\"AVG-GLOVE\", ft.partial(run_avg_benchmark, model=glove, use_stoplist=False)),\n",
    "              (\"AVG-GLOVE-STOP\", ft.partial(run_avg_benchmark, model=glove, use_stoplist=True)),\n",
    "              (\"AVG-GLOVE-TFIDF\", ft.partial(run_avg_benchmark, model=glove, use_stoplist=False, doc_freqs=doc_frequency)),\n",
    "              (\"AVG-GLOVE-TFIDF-STOP\",ft.partial(run_avg_benchmark, model=glove, use_stoplist=True, doc_freqs=doc_frequency))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method was tested for SICK dataset. Only the training sample was used. Pearson Correlation and Spearman Correlation was used between the actual similarity and predicted similarity to evaluate the benchmark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(25,25))\n",
    "row = 0\n",
    "column = 0\n",
    "sick_train = normalize(sick_train, [\"sim\"])\n",
    "for i in range(0, 8):\n",
    "    sims, topic = run_experiment(sick_train, benchmarks[i])\n",
    "    pearson_correlation = scipy.stats.pearsonr(sims, sick_train['sim'])[0]\n",
    "    spearman_correlation = scipy.stats.spearmanr(sims, sick_train['sim'])[0]\n",
    "    textstr = '$Pearson Correlation=%.2f$\\n$Spearman Correlation=%.2f$'%(pearson_correlation, spearman_correlation)\n",
    "    sick_train['predicted_sim'] = pd.Series(sims).values\n",
    "    sick_train = sick_train.sort_values('sim')\n",
    "    id = list(range(0, len(sick_train.index)))\n",
    "    sick_train['id'] = pd.Series(id).values\n",
    "    \n",
    "    if(i < 2):\n",
    "        row = 0\n",
    "        column = i\n",
    "    if(i >= 2 and i < 4):\n",
    "        row = 1\n",
    "        column = i-2\n",
    "    if(i >= 4 and i < 6):\n",
    "        row = 2\n",
    "        column = i-4\n",
    "    if(i >=6 and i < 8):\n",
    "        row = 3\n",
    "        column = i-6\n",
    "        \n",
    "    sick_train.plot(kind='scatter', x='id', y='sim',color='DarkBlue', label='Similarity', title = topic, ax=axes[row, column]);\n",
    "    sick_train.plot(kind='scatter', x='id', y='predicted_sim', color='DarkGreen', label='Predicted Similarity', ax=axes[row, column]);\n",
    "    axes[row, column].text(1500, 0.1, textstr, fontsize=12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method was tested using STS data set. Only the dev sample was used. Pearson Correlation and Spearman Correlation was calcluated as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(25,25))\n",
    "row = 0\n",
    "column = 0\n",
    "sts_dev = normalize(sts_dev, [\"sim\"])\n",
    "for i in range(0, 8):\n",
    "    sims, topic = run_experiment(sts_dev, benchmarks[i])\n",
    "    pearson_correlation = scipy.stats.pearsonr(sims, sts_dev['sim'])[0]\n",
    "    spearman_correlation = scipy.stats.spearmanr(sims, sts_dev['sim'])[0]\n",
    "    textstr = '$Pearson Correlation=%.2f$\\n$Spearman Correlation=%.2f$'%(pearson_correlation, spearman_correlation)\n",
    "    sts_dev['predicted_sim'] = pd.Series(sims).values\n",
    "    sts_dev = sts_dev.sort_values('sim')\n",
    "    id = list(range(0, len(sts_dev.index)))\n",
    "    sts_dev['id'] = pd.Series(id).values\n",
    "    \n",
    "    if(i < 2):\n",
    "        row = 0\n",
    "        column = i\n",
    "    if(i >= 2 and i < 4):\n",
    "        row = 1\n",
    "        column = i-2\n",
    "    if(i >= 4 and i < 6):\n",
    "        row = 2\n",
    "        column = i-4\n",
    "    if(i >=6 and i < 8):\n",
    "        row = 3\n",
    "        column = i-6\n",
    "        \n",
    "    sts_dev.plot(kind='scatter', x='id', y='sim',color='DarkBlue', label='Similarity', title = topic, ax=axes[row, column]);\n",
    "    sts_dev.plot(kind='scatter', x='id', y='predicted_sim', color='DarkGreen', label='Predicted Similarity', ax=axes[row, column]);\n",
    "    axes[row, column].text(800, 0.1, textstr, fontsize=14)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Mover's Distance\n",
    "Word mover's distance is a popular alternative to the simple average embedding similarity. The Word Mover's Distance uses the word embeddings of the words in two texts to measure the minimum amount that the words in one text need to \"travel\" in semantic space to reach the words of the other text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matrices.word_movers_distance import run_wmd_benchmark\n",
    "\n",
    "benchmarks = [(\"WMD-W2V\", ft.partial(run_wmd_benchmark, model=word2vec, use_stoplist=False)), \n",
    "              (\"WMD-W2V-STOP\", ft.partial(run_wmd_benchmark, model=word2vec, use_stoplist=True)), \n",
    "              (\"WMD-GLOVE\", ft.partial(run_wmd_benchmark, model=glove, use_stoplist=False)), \n",
    "              (\"WMD-GLOVE-STOP\", ft.partial(run_wmd_benchmark, model=glove, use_stoplist=True)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(25,25))\n",
    "row = 0\n",
    "column = 0\n",
    "sick_train = normalize(sick_train, [\"sim\"])\n",
    "for i in range(0, 4):\n",
    "    sims, topic = run_experiment(sick_train, benchmarks[i])\n",
    "    pearson_correlation = scipy.stats.pearsonr(sims, sick_train['sim'])[0]\n",
    "    spearman_correlation = scipy.stats.spearmanr(sims, sick_train['sim'])[0]\n",
    "    textstr = '$Pearson Correlation=%.2f$\\n$Spearman Correlation=%.2f$'%(pearson_correlation, spearman_correlation)\n",
    "    sick_train['predicted_sim'] = pd.Series(sims).values\n",
    "    sick_train = normalize(sick_train, [\"predicted_sim\"])\n",
    "    sick_train = sick_train.sort_values('sim')\n",
    "    id = list(range(0, len(sick_train.index)))\n",
    "    sick_train['id'] = pd.Series(id).values\n",
    "    \n",
    "    if(i < 2):\n",
    "        row = 0\n",
    "        column = i\n",
    "    if(i >= 2 and i < 4):\n",
    "        row = 1\n",
    "        column = i-2\n",
    "        \n",
    "    sick_train.plot(kind='scatter', x='id', y='sim',color='DarkBlue', label='Similarity', title = topic, ax=axes[row, column]);\n",
    "    sick_train.plot(kind='scatter', x='id', y='predicted_sim', color='DarkGreen', label='Predicted Similarity', ax=axes[row, column]);\n",
    "    axes[row, column].text(1500, 0.05, textstr, fontsize=12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scipy\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(25,25))\n",
    "row = 0\n",
    "column = 0\n",
    "sts_dev = normalize(sts_dev, [\"sim\"])\n",
    "for i in range(0, 4):\n",
    "    sims, topic = run_experiment(sts_dev, benchmarks[i])\n",
    "    pearson_correlation = scipy.stats.pearsonr(sims, sts_dev['sim'])[0]\n",
    "    spearman_correlation = scipy.stats.spearmanr(sims, sts_dev['sim'])[0]\n",
    "    textstr = '$Pearson Correlation=%.2f$\\n$Spearman Correlation=%.2f$'%(pearson_correlation, spearman_correlation)\n",
    "    sts_dev['predicted_sim'] = pd.Series(sims).values\n",
    "    sts_dev = normalize(sts_dev, [\"predicted_sim\"])\n",
    "    sts_dev = sts_dev.sort_values('sim')\n",
    "    id = list(range(0, len(sts_dev.index)))\n",
    "    sts_dev['id'] = pd.Series(id).values\n",
    "    \n",
    "    if(i < 2):\n",
    "        row = 0\n",
    "        column = i\n",
    "    if(i >= 2 and i < 4):\n",
    "        row = 1\n",
    "        column = i-2\n",
    "        \n",
    "    sts_dev.plot(kind='scatter', x='id', y='sim',color='DarkBlue', label='Similarity', title = topic, ax=axes[row, column]);\n",
    "    sts_dev.plot(kind='scatter', x='id', y='predicted_sim', color='DarkGreen', label='Predicted Similarity', ax=axes[row, column]);\n",
    "    axes[row, column].text(800, 0.05, textstr, fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth Inverse Frequency\n",
    "Taking the average of the word embeddings in a sentence, like we did above, is a very crude method of computing sentence embeddings. Most importantly, this gives far too much weight to words that are quite irrelevant, semantically speaking. Smooth Inverse Frequency tries to solve this problem.\n",
    "\n",
    "To compute SIF sentence embeddings, we first compute a weighted average of the token embeddings in the sentence. This procedure is very similar to the weighted average we used above, with the single difference that the word embeddings are weighted by a/a+p(w), where w is a parameter that is set to 0.001 by default, and p(w) is the estimated relative frequency of a word in a reference corpus.\n",
    "\n",
    "Next, we need to perform common component removal: we compute the principal component of the sentence embeddings we obtained above and subtract from them their projections on this first principal component. This corrects for the influence of high-frequency words that mostly have a syntactic or discourse function, such as \"just\", \"there\", \"but\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = [(\"SIF-W2V\", ft.partial(run_sif_benchmark, freqs=frequencies, model=word2vec, use_stoplist=False)),\n",
    "              (\"SIF-GLOVE\", ft.partial(run_sif_benchmark, freqs=frequencies, model=glove, use_stoplist=False))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(25,25))\n",
    "row = 0\n",
    "column = 0\n",
    "sick_train = normalize(sick_train, [\"sim\"])\n",
    "for i in range(0, 4):\n",
    "    sims, topic = run_experiment(sick_train, benchmarks[i])\n",
    "    pearson_correlation = scipy.stats.pearsonr(sims, sick_train['sim'])[0]\n",
    "    spearman_correlation = scipy.stats.spearmanr(sims, sick_train['sim'])[0]\n",
    "    textstr = '$Pearson Correlation=%.2f$\\n$Spearman Correlation=%.2f$'%(pearson_correlation, spearman_correlation)\n",
    "    sick_train['predicted_sim'] = pd.Series(sims).values\n",
    "    sick_train = normalize(sick_train, [\"predicted_sim\"])\n",
    "    sick_train = sick_train.sort_values('sim')\n",
    "    id = list(range(0, len(sick_train.index)))\n",
    "    sick_train['id'] = pd.Series(id).values\n",
    "    \n",
    "    if(i < 2):\n",
    "        row = 0\n",
    "        column = i\n",
    "    if(i >= 2 and i < 4):\n",
    "        row = 1\n",
    "        column = i-2\n",
    "        \n",
    "    sick_train.plot(kind='scatter', x='id', y='sim',color='DarkBlue', label='Similarity', title = topic, ax=axes[row, column]);\n",
    "    sick_train.plot(kind='scatter', x='id', y='predicted_sim', color='DarkGreen', label='Predicted Similarity', ax=axes[row, column]);\n",
    "    axes[row, column].text(1500, 0.05, textstr, fontsize=12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,10))\n",
    "row = 0\n",
    "column = 0\n",
    "sts_dev = normalize(sts_dev, [\"sim\"])\n",
    "for i in range(0, 2):\n",
    "    sims, topic = run_experiment(sts_dev, benchmarks[i])\n",
    "    pearson_correlation = scipy.stats.pearsonr(sims, sts_dev['sim'])[0]\n",
    "    spearman_correlation = scipy.stats.spearmanr(sims, sts_dev['sim'])[0]\n",
    "    textstr = '$Pearson Correlation=%.2f$\\n$Spearman Correlation=%.2f$'%(pearson_correlation, spearman_correlation)\n",
    "    sts_dev['predicted_sim'] = pd.Series(sims).values\n",
    "    sts_dev = normalize(sts_dev, [\"predicted_sim\"])\n",
    "    sts_dev = sts_dev.sort_values('sim')\n",
    "    id = list(range(0, len(sts_dev.index)))\n",
    "    sts_dev['id'] = pd.Series(id).values\n",
    "    \n",
    "    if(i < 2):\n",
    "        column = i\n",
    "        \n",
    "    sts_dev.plot(kind='scatter', x='id', y='sim',color='DarkBlue', label='Similarity', title = topic, ax=axes[column]);\n",
    "    sts_dev.plot(kind='scatter', x='id', y='predicted_sim', color='DarkGreen', label='Predicted Similarity', ax=axes[column]);\n",
    "    axes[column].text(800, 0.05, textstr, fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InferSent\n",
    "InferSent is a pre-trained encoder that produces sentence embeddings. More particularly, it is a BiLSTM with max pooling that was trained on the SNLI dataset, 570k English sentence pairs labelled with one of three categories: entailment, contradiction or neutral. InferSent was developed and trained by Facebook Research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utility.infer_sent.models import InferSent\n",
    "\n",
    "\n",
    "MODEL_PATH = \"/data/InferSent/encoder/infersent1.pkl\"\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': 1}\n",
    "model = InferSent(params_model)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "GLOVE_PATH = '/data/glove/glove.840B.300d.txt'\n",
    "model.set_w2v_path(GLOVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matrices.infer_sent import run_inf_benchmark\n",
    "from matrices.google_sentence_encoder import run_gse_benchmark\n",
    "import functools as ft\n",
    "\n",
    "benchmarks = [ (\"INF\", ft.partial(run_inf_benchmark, infersent=model)), \n",
    "             (\"GSE\", ft.partial(run_gse_benchmark, embed=model))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(25,25))\n",
    "row = 0\n",
    "column = 0\n",
    "sick_train = normalize(sick_train, [\"sim\"])\n",
    "for i in range(0, 2):\n",
    "    sims, topic = run_experiment(sick_train, benchmarks[i])\n",
    "    pearson_correlation = scipy.stats.pearsonr(sims, sick_train['sim'])[0]\n",
    "    spearman_correlation = scipy.stats.spearmanr(sims, sick_train['sim'])[0]\n",
    "    textstr = '$Pearson Correlation=%.2f$\\n$Spearman Correlation=%.2f$'%(pearson_correlation, spearman_correlation)\n",
    "    sick_train['predicted_sim'] = pd.Series(sims).values\n",
    "    sick_train = normalize(sick_train, [\"predicted_sim\"])\n",
    "    sick_train = sick_train.sort_values('sim')\n",
    "    id = list(range(0, len(sick_train.index)))\n",
    "    sick_train['id'] = pd.Series(id).values\n",
    "    \n",
    "    if(i < 2):\n",
    "        row = 0\n",
    "        column = i\n",
    "    if(i >= 2 and i < 4):\n",
    "        row = 1\n",
    "        column = i-2\n",
    "        \n",
    "    sick_train.plot(kind='scatter', x='id', y='sim',color='DarkBlue', label='Similarity', title = topic, ax=axes[row, column]);\n",
    "    sick_train.plot(kind='scatter', x='id', y='predicted_sim', color='DarkGreen', label='Predicted Similarity', ax=axes[row, column]);\n",
    "    axes[row, column].text(1500, 0.05, textstr, fontsize=12)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple_sentence_similarity_3.6",
   "language": "python",
   "name": "simple_sentence_similarity_3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
